# ğŸš€ Documentation ComplÃ¨te - DÃ©ploiement LLM Local avec Kubernetes

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [PrÃ©requis](#prÃ©requis)
3. [Architecture](#architecture)
4. [Installation et Configuration](#installation-et-configuration)
5. [DÃ©marrage Rapide](#dÃ©marrage-rapide)
6. [Gestion et Maintenance](#gestion-et-maintenance)
7. [Tests et Validation](#tests-et-validation)
8. [DÃ©pannage](#dÃ©pannage)
9. [Optimisation](#optimisation)
10. [Sauvegarde et Restauration](#sauvegarde-et-restauration)

---

## ğŸ¯ Vue d'ensemble

Cette documentation dÃ©crit le dÃ©ploiement d'un LLM local (Gemma 3-1B) sur Kubernetes avec une interface web (OpenWebUI) pour interagir avec le modÃ¨le.

### Composants

- **Gemma 3-1B** : ModÃ¨le de langage Google de 1 milliard de paramÃ¨tres
- **vLLM** : Moteur d'infÃ©rence haute performance
- **OpenWebUI** : Interface web pour interagir avec le modÃ¨le
- **Kubernetes** : Orchestration et gestion des conteneurs

### URLs d'accÃ¨s

| Service | URL | Description |
|---------|-----|-------------|
| API Gemma3 | `http://192.168.1.230:30180` | API REST compatible OpenAI |
| OpenWebUI | `http://192.168.1.230:30183` | Interface web utilisateur |

---

## ğŸ”§ PrÃ©requis

### MatÃ©riel

- **GPU** : 1x NVIDIA GPU avec au moins 8Go VRAM
- **RAM** : Minimum 16Go (recommandÃ© 32Go)
- **CPU** : Minimum 8 cores
- **Stockage** : Minimum 50Go disponible

### Logiciels

- **Kubernetes** : v1.20+
- **kubectl** : InstallÃ© et configurÃ©
- **NVIDIA GPU Operator** : Pour le support GPU
- **StorageClass** : `local-path` ou Ã©quivalent

### VÃ©rifications prÃ©alables

```bash
# VÃ©rifier kubectl
kubectl version --client

# VÃ©rifier l'accÃ¨s au cluster
kubectl get nodes

# VÃ©rifier le support GPU
kubectl get nodes -o json | jq '.items[].status.allocatable."nvidia.com/gpu"'

# VÃ©rifier la StorageClass
kubectl get storageclass
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Utilisateur                    â”‚
â”‚              (Navigateur Web)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ HTTP :30183
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            OpenWebUI Service                    â”‚
â”‚              (NodePort)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           OpenWebUI Pod                         â”‚
â”‚         (Interface Web)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ HTTP :8000
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Gemma3 Service                       â”‚
â”‚              (ClusterIP)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Gemma3-1B Pod                        â”‚
â”‚         (vLLM + GPU)                            â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  PVC: gemma3-cache (30Gi)   â”‚               â”‚
â”‚  â”‚  /root/.cache/huggingface   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Installation et Configuration

### Ã‰tape 1 : CrÃ©er le namespace

```bash
kubectl create namespace vllm-chat
```

### Ã‰tape 2 : CrÃ©er le secret HuggingFace (si nÃ©cessaire)

```bash
# Remplacer YOUR_HF_TOKEN par votre token HuggingFace
kubectl create secret generic hf-token-secret \
  --from-literal=token='YOUR_HF_TOKEN' \
  -n vllm-chat
```

### Ã‰tape 3 : CrÃ©er les fichiers de configuration

CrÃ©ez les fichiers YAML suivants dans un dossier `~/vLLM_Deploy/` :

#### `gemma3-pvc.yaml`

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gemma3-cache
  namespace: vllm-chat
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 30Gi
```

#### `gemma3-service.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: gemma3-service
  namespace: vllm-chat
spec:
  selector:
    app: gemma3-1b
  ports:
  - name: http
    port: 8000
    targetPort: 8000
    nodePort: 30180
  type: NodePort
```

#### `gemma3-deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gemma3-1b
  namespace: vllm-chat
  labels:
    app: gemma3-1b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gemma3-1b
  template:
    metadata:
      labels:
        app: gemma3-1b
    spec:
      volumes:
      - name: cache-volume
        persistentVolumeClaim:
          claimName: gemma3-cache
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: "4Gi"
      containers:
      - name: gemma3-1b
        image: vllm/vllm-openai:v0.10.0
        command: ["/bin/sh", "-c"]
        args: [
          "vllm serve google/gemma-3-1b-it --dtype=float16 --tensor-parallel-size 1 --max-model-len 4096 --max-num-batched-tokens 4096 --trust-remote-code --gpu-memory-utilization 0.85"
        ]
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token-secret
              key: token
        ports:
        - containerPort: 8000
        resources:
          limits:
            cpu: "8"
            memory: 16G
            nvidia.com/gpu: "1"
          requests:
            cpu: "2"
            memory: 4G
            nvidia.com/gpu: "1"
        volumeMounts:
        - mountPath: /root/.cache/huggingface
          name: cache-volume
        - name: shm
          mountPath: /dev/shm
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3
```

#### `openwebui-pvc.yaml`

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: openwebui-data
  namespace: vllm-chat
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 10Gi
```

#### `openwebui-deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: openwebui
  namespace: vllm-chat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: openwebui
  template:
    metadata:
      labels:
        app: openwebui
    spec:
      containers:
      - name: openwebui
        image: ghcr.io/open-webui/open-webui:main
        ports:
        - containerPort: 8080
        env:
        - name: OPENAI_API_BASE_URL
          value: "http://gemma3-service:8000/v1"
        - name: OPENAI_API_KEY
          value: "sk-dummy"
        - name: WEBUI_NAME
          value: "Gemma 3 Chat"
        volumeMounts:
        - name: webui-data
          mountPath: /app/backend/data
      volumes:
      - name: webui-data
        persistentVolumeClaim:
          claimName: openwebui-data
---
apiVersion: v1
kind: Service
metadata:
  name: openwebui-service
  namespace: vllm-chat
spec:
  selector:
    app: openwebui
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    nodePort: 30183
  type: NodePort
```

---

## ğŸš€ DÃ©marrage Rapide

### DÃ©ploiement complet depuis zÃ©ro

```bash
# 1. CrÃ©er le namespace
kubectl create namespace vllm-chat

# 2. CrÃ©er le secret HuggingFace
kubectl create secret generic hf-token-secret \
  --from-literal=token='YOUR_HF_TOKEN' \
  -n vllm-chat

# 3. DÃ©ployer dans l'ordre
cd ~/vLLM_Deploy

# PVC Gemma3
kubectl apply -f gemma3-pvc.yaml

# Attendre que le PVC soit Bound
kubectl get pvc gemma3-cache -n vllm-chat -w
# Appuyer sur Ctrl+C quand STATUS = Bound

# Service Gemma3
kubectl apply -f gemma3-service.yaml

# DÃ©ploiement Gemma3
kubectl apply -f gemma3-deployment.yaml

# PVC OpenWebUI
kubectl apply -f openwebui-pvc.yaml

# Attendre que le PVC soit Bound
kubectl get pvc openwebui-data -n vllm-chat -w
# Appuyer sur Ctrl+C quand STATUS = Bound

# OpenWebUI
kubectl apply -f openwebui-deployment.yaml

# 4. Surveiller le dÃ©ploiement
kubectl get pods -n vllm-chat -w
```

### Temps de dÃ©marrage attendus

- **Gemma3** : ~5-7 minutes (tÃ©lÃ©chargement + chargement du modÃ¨le)
- **OpenWebUI** : ~30 secondes

### VÃ©rification du dÃ©ploiement

```bash
# VÃ©rifier que tous les pods sont Running
kubectl get pods -n vllm-chat

# RÃ©sultat attendu :
# NAME                         READY   STATUS    RESTARTS   AGE
# gemma3-1b-xxxxx-xxxxx        1/1     Running   0          7m
# openwebui-xxxxx-xxxxx        1/1     Running   0          2m

# VÃ©rifier les services
kubectl get svc -n vllm-chat

# Tester l'API Gemma3
curl http://192.168.1.230:30180/health

# Tester OpenWebUI
curl -I http://192.168.1.230:30183
```

---

## ğŸ”„ Gestion et Maintenance

### DÃ©marrer les services

```bash
# Si les dÃ©ploiements existent mais sont Ã  0 replicas
kubectl scale deployment gemma3-1b -n vllm-chat --replicas=1
kubectl scale deployment openwebui -n vllm-chat --replicas=1
```

### ArrÃªter les services

```bash
# Mettre les replicas Ã  0 (conserve la configuration)
kubectl scale deployment gemma3-1b -n vllm-chat --replicas=0
kubectl scale deployment openwebui -n vllm-chat --replicas=0

# VÃ©rifier que les pods sont arrÃªtÃ©s
kubectl get pods -n vllm-chat
```

### RedÃ©marrer un service

```bash
# RedÃ©marrer Gemma3
kubectl rollout restart deployment/gemma3-1b -n vllm-chat

# RedÃ©marrer OpenWebUI
kubectl rollout restart deployment/openwebui -n vllm-chat

# Surveiller le redÃ©marrage
kubectl rollout status deployment/gemma3-1b -n vllm-chat
```

### Voir les logs

```bash
# Logs Gemma3 en temps rÃ©el
kubectl logs -n vllm-chat -l app=gemma3-1b -f

# Logs OpenWebUI en temps rÃ©el
kubectl logs -n vllm-chat -l app=openwebui -f

# Derniers 100 logs
kubectl logs -n vllm-chat -l app=gemma3-1b --tail=100
```

### Surveiller les ressources

```bash
# Utilisation CPU/RAM des pods
kubectl top pods -n vllm-chat

# DÃ©tails d'un pod
kubectl describe pod -n vllm-chat <pod-name>

# Ã‰vÃ©nements rÃ©cents
kubectl get events -n vllm-chat --sort-by='.lastTimestamp' | tail -20
```

---

## ğŸ§ª Tests et Validation

### Test de l'API Gemma3

```bash
# Test de santÃ©
curl http://192.168.1.230:30180/health

# Lister les modÃ¨les
curl http://192.168.1.230:30180/v1/models

# Test de gÃ©nÃ©ration simple
curl http://192.168.1.230:30180/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "google/gemma-3-1b-it",
    "messages": [
      {"role": "user", "content": "Bonjour, prÃ©sente-toi en une phrase."}
    ],
    "max_tokens": 50,
    "temperature": 0.7
  }'
```

### Test de gÃ©nÃ©ration avec streaming

```bash
curl http://192.168.1.230:30180/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "google/gemma-3-1b-it",
    "messages": [
      {"role": "user", "content": "Ã‰cris un court poÃ¨me sur l'\''IA."}
    ],
    "max_tokens": 100,
    "stream": true
  }'
```

### Benchmark de performance

```bash
# Installation de l'outil de benchmark (si disponible)
pip install vllm

# Lancer un benchmark
python -m vllm.entrypoints.openai.api_server \
  --model google/gemma-3-1b-it \
  --benchmark
```

---

## ğŸ”§ DÃ©pannage

### ProblÃ¨me : Pod en CrashLoopBackOff

**Diagnostic :**
```bash
kubectl logs -n vllm-chat <pod-name> --previous
kubectl describe pod -n vllm-chat <pod-name>
```

**Solutions courantes :**
- VÃ©rifier que le GPU est disponible : `kubectl get nodes -o json | jq '.items[].status.allocatable."nvidia.com/gpu"'`
- VÃ©rifier l'espace disque du PVC
- VÃ©rifier le token HuggingFace

### ProblÃ¨me : Pod en Pending

**Diagnostic :**
```bash
kubectl describe pod -n vllm-chat <pod-name>
```

**Solutions courantes :**
- PVC non Bound : `kubectl get pvc -n vllm-chat`
- Pas de GPU disponible : RÃ©duire les replicas ou libÃ©rer un GPU
- Resources insuffisantes : VÃ©rifier `kubectl describe nodes`

### ProblÃ¨me : Pod Running mais pas Ready (0/1)

**Diagnostic :**
```bash
kubectl logs -n vllm-chat <pod-name> -f
```

**Explication :**
- Pour Gemma3, c'est normal pendant 5-7 minutes (tÃ©lÃ©chargement + chargement)
- Les readiness probes ont un `initialDelaySeconds: 300`

### ProblÃ¨me : Connection refused sur NodePort

**Diagnostic :**
```bash
# VÃ©rifier les endpoints
kubectl get endpoints -n vllm-chat

# VÃ©rifier le service
kubectl get svc -n vllm-chat

# Tester depuis l'intÃ©rieur du cluster
kubectl run test --rm -it --image=curlimages/curl --restart=Never -n vllm-chat -- \
  curl http://gemma3-service:8000/health
```

**Solutions :**
- VÃ©rifier que le pod est Ready (1/1)
- VÃ©rifier le pare-feu : `sudo ufw allow 30180/tcp && sudo ufw allow 30183/tcp`
- Tester avec l'IP locale : `curl http://127.0.0.1:30180/health`

### ProblÃ¨me : ModÃ¨le ne se tÃ©lÃ©charge pas

**Diagnostic :**
```bash
kubectl logs -n vllm-chat -l app=gemma3-1b | grep -i "download\|error"
```

**Solutions :**
- VÃ©rifier le token HuggingFace
- VÃ©rifier la connectivitÃ© internet du pod
- VÃ©rifier l'espace disque du PVC : `kubectl describe pvc gemma3-cache -n vllm-chat`

### ProblÃ¨me : Out of Memory (OOM)

**SymptÃ´mes :**
```bash
kubectl get pods -n vllm-chat
# Pod en Ã©tat OOMKilled
```

**Solutions :**
```bash
# RÃ©duire l'utilisation GPU
kubectl set env deployment/gemma3-1b -n vllm-chat \
  VLLM_GPU_MEMORY_UTILIZATION=0.7

# Ou Ã©diter le dÃ©ploiement
kubectl edit deployment gemma3-1b -n vllm-chat
# Modifier --gpu-memory-utilization 0.85 â†’ 0.7
```

---

## âš¡ Optimisation

### Ajuster l'utilisation de la mÃ©moire GPU

```bash
# Ã‰diter le dÃ©ploiement
kubectl edit deployment gemma3-1b -n vllm-chat

# Modifier la ligne dans args:
# --gpu-memory-utilization 0.85 â†’ 0.9 (plus agressif)
# --gpu-memory-utilization 0.85 â†’ 0.7 (plus conservateur)
```

### Augmenter la longueur de contexte

```bash
kubectl edit deployment gemma3-1b -n vllm-chat

# Modifier dans args:
# --max-model-len 4096 â†’ 8192
# --max-num-batched-tokens 4096 â†’ 8192
```

### Activer le multi-GPU (si disponible)

```bash
# Ã‰diter le dÃ©ploiement
kubectl edit deployment gemma3-1b -n vllm-chat

# Modifier:
# --tensor-parallel-size 1 â†’ 2 (pour 2 GPUs)
# resources.limits.nvidia.com/gpu: "1" â†’ "2"
# resources.requests.nvidia.com/gpu: "1" â†’ "2"
```

### Scaler horizontalement (Load Balancing)

```bash
# Augmenter le nombre de replicas
kubectl scale deployment gemma3-1b -n vllm-chat --replicas=2

# Note: NÃ©cessite 2 GPUs disponibles
```

---

## ğŸ’¾ Sauvegarde et Restauration

### Sauvegarder la configuration

```bash
# CrÃ©er un dossier de sauvegarde
mkdir -p ~/vLLM_Deploy/backup_$(date +%Y%m%d)

# Exporter toutes les ressources
kubectl get all -n vllm-chat -o yaml > ~/vLLM_Deploy/backup_$(date +%Y%m%d)/all-resources.yaml
kubectl get pvc -n vllm-chat -o yaml > ~/vLLM_Deploy/backup_$(date +%Y%m%d)/pvcs.yaml
kubectl get secret hf-token-secret -n vllm-chat -o yaml > ~/vLLM_Deploy/backup_$(date +%Y%m%d)/secrets.yaml
kubectl get configmap -n vllm-chat -o yaml > ~/vLLM_Deploy/backup_$(date +%Y%m%d)/configmaps.yaml
```

### Sauvegarder les donnÃ©es

```bash
# Sauvegarder le cache du modÃ¨le (optionnel, peut Ãªtre re-tÃ©lÃ©chargÃ©)
# Note: Cela peut Ãªtre volumineux (plusieurs Go)

# CrÃ©er un pod temporaire pour accÃ©der au PVC
kubectl run backup-pod --rm -it --image=busybox -n vllm-chat \
  --overrides='
  {
    "spec": {
      "containers": [{
        "name": "backup",
        "image": "busybox",
        "command": ["sleep", "3600"],
        "volumeMounts": [{
          "name": "cache",
          "mountPath": "/cache"
        }]
      }],
      "volumes": [{
        "name": "cache",
        "persistentVolumeClaim": {
          "claimName": "gemma3-cache"
        }
      }]
    }
  }' -- sh

# Dans le pod, crÃ©er une archive
tar czf /cache/backup.tar.gz /cache/models
exit

# Copier l'archive hors du pod
kubectl cp vllm-chat/backup-pod:/cache/backup.tar.gz ~/vLLM_Deploy/backup_$(date +%Y%m%d)/model-cache.tar.gz
```

### Restaurer depuis une sauvegarde

```bash
# Restaurer les ressources
cd ~/vLLM_Deploy/backup_YYYYMMDD

kubectl apply -f secrets.yaml
kubectl apply -f pvcs.yaml
kubectl apply -f all-resources.yaml
```

---

## ğŸ“Š Monitoring et MÃ©triques

### MÃ©triques Prometheus (si installÃ©)

```bash
# AccÃ©der aux mÃ©triques vLLM
curl http://192.168.1.230:30180/metrics

# MÃ©triques disponibles :
# - vllm_num_requests_running
# - vllm_num_requests_waiting
# - vllm_gpu_cache_usage_perc
# - vllm_time_to_first_token_seconds
# - vllm_time_per_output_token_seconds
```

### Dashboard Kubernetes

```bash
# Si Kubernetes Dashboard est installÃ©
kubectl proxy
# AccÃ©der via http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
```

---

## ğŸ” SÃ©curitÃ©

### Bonnes pratiques

1. **Ne pas exposer les NodePorts publiquement**
   - Utiliser un Ingress avec TLS
   - Configurer un VPN ou un bastion

2. **Changer le token API OpenWebUI**
   ```bash
   kubectl set env deployment/openwebui -n vllm-chat \
     OPENAI_API_KEY="votre-token-securise"
   ```

3. **Activer l'authentification OpenWebUI**
   - Premier utilisateur = admin
   - Configurer les rÃ´les et permissions

4. **Limiter les ressources**
   - Les limits/requests empÃªchent l'Ã©puisement des ressources

### Mise Ã  jour des images

```bash
# Mettre Ã  jour vLLM
kubectl set image deployment/gemma3-1b -n vllm-chat \
  gemma3-1b=vllm/vllm-openai:v0.11.0

# Mettre Ã  jour OpenWebUI
kubectl set image deployment/openwebui -n vllm-chat \
  openwebui=ghcr.io/open-webui/open-webui:latest
```

---

## ğŸ“š Ressources supplÃ©mentaires

### Documentation officielle

- **vLLM** : https://docs.vllm.ai/
- **OpenWebUI** : https://docs.openwebui.com/
- **Gemma** : https://ai.google.dev/gemma
- **Kubernetes** : https://kubernetes.io/docs/

### CommunautÃ©

- **vLLM GitHub** : https://github.com/vllm-project/vllm
- **OpenWebUI GitHub** : https://github.com/open-webui/open-webui

---

## ğŸ†˜ Support

En cas de problÃ¨me :

1. VÃ©rifier les logs : `kubectl logs -n vllm-chat -l app=gemma3-1b --tail=100`
2. VÃ©rifier les Ã©vÃ©nements : `kubectl get events -n vllm-chat`
3. Consulter la section DÃ©pannage ci-dessus
4. Rechercher dans les issues GitHub des projets

---

## ğŸ“ Changelog

### v1.0 (2026-01-04)
- DÃ©ploiement initial Gemma 3-1B
- Configuration OpenWebUI
- Documentation complÃ¨te

---

**Auteur** : Documentation gÃ©nÃ©rÃ©e le 2026-01-04  
**Version** : 1.0  
**DerniÃ¨re mise Ã  jour** : 2026-01-04