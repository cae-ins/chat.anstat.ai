# Fine-tuning LoRA de Phi-3.5-mini pour ANSTAT AI

Ce dossier contient tout le n√©cessaire pour fine-tuner le mod√®le Phi-3.5-mini avec les m√©thodologies statistiques de l'ANSTAT.

## Structure du dossier

```
finetuning/
‚îú‚îÄ‚îÄ train_lora.py               # Script principal de fine-tuning
‚îú‚îÄ‚îÄ merge_lora.py               # Script de fusion LoRA + mod√®le base
‚îú‚îÄ‚îÄ parse_methodologies.py      # Script de parsing des documents
‚îú‚îÄ‚îÄ Dockerfile                  # Image Docker pour le fine-tuning
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances Python
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ training_config.yaml    # Configuration des hyperparam√®tres
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ methodologies_anstat.jsonl  # Donn√©es d'entra√Ænement
‚îú‚îÄ‚îÄ methodologies_sources/      # ‚¨ÖÔ∏è PLACEZ VOS DOCUMENTS ICI
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îî‚îÄ‚îÄ k8s/
    ‚îú‚îÄ‚îÄ finetuning-job.yaml         # Job Kubernetes pour fine-tuning
    ‚îî‚îÄ‚îÄ phi3-anstat-deployment.yaml # D√©ploiement du mod√®le fine-tun√©
```

## Pr√©requis

- GPU NVIDIA avec au moins 16 Go VRAM (T4, A100, etc.)
- CUDA 12.1+
- Python 3.10+
- Token HuggingFace (pour t√©l√©charger Phi-3.5)

## Installation locale

```bash
# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Installer PyTorch avec CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Installer les d√©pendances
pip install -r requirements.txt

# Installer flash-attention (optionnel mais recommand√©)
pip install flash-attn --no-build-isolation
```

## Parsing automatique des documents

Placez vos documents (PDF, DOCX, TXT, MD) dans `methodologies_sources/` puis lancez :

```bash
pip install pymupdf python-docx
python parse_methodologies.py
```

Le script extrait automatiquement les sections et g√©n√®re des paires question/r√©ponse.

üìñ **Documentation compl√®te : [parsing.md](parsing.md)**

---

## Pr√©paration manuelle des donn√©es (optionnel)

### Format des donn√©es

Les donn√©es doivent √™tre au format JSONL avec la structure suivante :

```json
{"instruction": "Question sur une m√©thodologie", "input": "", "output": "R√©ponse d√©taill√©e"}
```

### Exemple

```json
{"instruction": "Comment calcule-t-on l'IPC ?", "input": "", "output": "L'IPC est calcul√© selon la formule de Laspeyres..."}
```

### Enrichir les donn√©es

1. Ouvrir `data/methodologies_anstat.jsonl`
2. Ajouter vos propres exemples de m√©thodologies
3. Recommandation : minimum 50-100 exemples pour de bons r√©sultats

## Lancer le fine-tuning

### Option 1 : En local (GPU requis)

```bash
# Fine-tuning basique
python train_lora.py \
    --data_path ./data/methodologies_anstat.jsonl \
    --output_dir ./output/phi3-anstat-lora \
    --num_epochs 3 \
    --batch_size 4

# Fine-tuning avec QLoRA (pour GPU 16 Go comme T4)
python train_lora.py \
    --data_path ./data/methodologies_anstat.jsonl \
    --output_dir ./output/phi3-anstat-lora \
    --num_epochs 3 \
    --batch_size 2 \
    --use_4bit \
    --gradient_checkpointing
```

### Option 2 : Sur Kubernetes

```bash
# Construire l'image Docker
docker build -t anstat/phi3-finetuning:latest .

# Pousser vers votre registry
docker push anstat/phi3-finetuning:latest

# Copier les donn√©es vers le PVC
kubectl cp ./data/methodologies_anstat.jsonl vllm-chat/pod-name:/data/

# Lancer le job de fine-tuning
kubectl apply -f k8s/finetuning-job.yaml -n vllm-chat

# Suivre les logs
kubectl logs -f job/phi3-finetuning-lora -n vllm-chat
```

## Fusion des poids LoRA

Apr√®s le fine-tuning, fusionner les adaptateurs LoRA avec le mod√®le de base :

```bash
python merge_lora.py \
    --lora_path ./output/phi3-anstat-lora \
    --output_path ./output/phi3-anstat-merged
```

## D√©ploiement du mod√®le fine-tun√©

### Sur Kubernetes

```bash
# Copier le mod√®le fusionn√© vers le PVC
kubectl cp ./output/phi3-anstat-merged vllm-chat/pod-name:/models/

# D√©ployer
kubectl apply -f k8s/phi3-anstat-deployment.yaml -n vllm-chat
```

### Mise √† jour d'OpenWebUI

Configurer OpenWebUI pour pointer vers le nouveau service :
- URL : `http://phi3-anstat-service:8000/v1`
- Nom du mod√®le : `phi3-anstat`

## Configurations par GPU

### NVIDIA T4 (16 Go VRAM)

```bash
python train_lora.py \
    --use_4bit \
    --batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lora_r 8 \
    --gradient_checkpointing
```

### NVIDIA A100 40GB

```bash
python train_lora.py \
    --batch_size 8 \
    --gradient_accumulation_steps 2 \
    --lora_r 32 \
    --lora_alpha 64
```

### NVIDIA A100 80GB / 4x A100

```bash
python train_lora.py \
    --batch_size 16 \
    --lora_r 64 \
    --lora_alpha 128
```

## Hyperparam√®tres recommand√©s

| Param√®tre | Valeur | Description |
|-----------|--------|-------------|
| `lora_r` | 16 | Rang de la d√©composition LoRA |
| `lora_alpha` | 32 | Facteur de scaling (2x le rang) |
| `lora_dropout` | 0.05 | Dropout pour r√©gularisation |
| `learning_rate` | 2e-4 | Taux d'apprentissage |
| `num_epochs` | 3 | Nombre d'√©poques |
| `batch_size` | 4 | Taille du batch (ajuster selon GPU) |

## V√©rification du mod√®le

Apr√®s fusion, tester le mod√®le :

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("./output/phi3-anstat-merged")
tokenizer = AutoTokenizer.from_pretrained("./output/phi3-anstat-merged")

prompt = "<|user|>\nComment calcule-t-on l'IPC √† l'ANSTAT ?<|end|>\n<|assistant|>\n"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
```

## Troubleshooting

### Erreur CUDA Out of Memory

- R√©duire `batch_size`
- Activer `--use_4bit` (QLoRA)
- Activer `--gradient_checkpointing`
- R√©duire `lora_r`

### Mod√®le ne converge pas

- Augmenter `num_epochs`
- Ajuster `learning_rate` (essayer 1e-4 ou 5e-5)
- V√©rifier la qualit√© des donn√©es

### Flash Attention non disponible

Le script fonctionne sans flash-attention mais sera plus lent. Pour l'installer :

```bash
pip install flash-attn --no-build-isolation
```

## Ressources

- [Documentation PEFT](https://huggingface.co/docs/peft)
- [Phi-3 sur HuggingFace](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)
- [Guide LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)

## Support

Centre de Calcul CAE & DataLab ANSTAT
Email : cae@stat.plan.gouv.ci
